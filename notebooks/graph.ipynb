{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import calendar\n",
    "import dill\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "In this project, we are going to be crawling through the [Party Picture Archive](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures).\n",
    "\n",
    "We outline the steps in this project as follows.\n",
    "\n",
    "1. Visiting [this url](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures) returns a list of events, with each linking to a seperate URL detailing each event. We will scrap the page of those URLS. Note there are 25 pages in total.\n",
    "2. Each event contains several pictures, with associated captions. We shall scrap those captions for names.\n",
    "3. From those names, we shall build a graph. The nodes (or vertices) are names, the (weighted) edges refer to how many photos two individuals appeared together.\n",
    "\n",
    "From there, we use various elements of graph theory to study relationships between individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Index\n",
    "\n",
    "\n",
    "### Core Functions\n",
    "We need to script a means of processing each page. Let's start with the first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures'\n",
    "page = requests.get(url) # Use requests.get to download the page.\n",
    "soup = BeautifulSoup(page.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to grab the links in each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_re_pattern = re.compile('.*/party-pictures/\\d{4}/.*')\n",
    "\n",
    "links = soup.find_all('a', href=True)\n",
    "links = [link for link in links if re.search(party_re_pattern, link['href'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_name_to_number = dict()\n",
    "i = 0\n",
    "\n",
    "for month in calendar.month_name:\n",
    "    month_name_to_number[month] = i\n",
    "    i += 1\n",
    "\n",
    "\n",
    "def get_link_date(el):\n",
    "    \"\"\"\n",
    "    Keyword Args:\n",
    "    el - BS4 tagged linked from the Party Picture Archive.\n",
    "    \n",
    "    Returns:\n",
    "    - URL of link.\n",
    "    - Date link tagged as.\n",
    "    \"\"\"\n",
    "    url = el['href']\n",
    "    date_pattern = re.compile('(.*), (.*) (\\d{1,2}), (\\d{4})')\n",
    "    date_match = re.match(date_pattern, list(el.parent.parent.parent.children)[3].text)\n",
    "\n",
    "    _, month, day, year = date_match[1], date_match[2], date_match[3], date_match[4]\n",
    "    month = month_name_to_number[month]\n",
    "    day = int(day)\n",
    "    year = int(year)\n",
    "    date = datetime(year, month, day)\n",
    "    return url, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures/2015/kicks-offs-sing-offs-and-pro-ams',\n",
       " datetime.datetime(2015, 9, 11, 0, 0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_link_date(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(response):\n",
    "    \"\"\"\n",
    "    Used to parse all (url, date) pairs inside a given given address.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    response - Either a URL or requests response of an index page.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of (url, date) pairs linked to inside the address.\n",
    "    \"\"\"\n",
    "    if isinstance(response, str):\n",
    "        response = requests.get(response)\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    links = soup.find_all('a', href=True)\n",
    "    links = [link for link in links if re.search(party_re_pattern, link['href'])]\n",
    "    return [get_link_date(link) for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_date(links, cutoff=datetime(2014, 12, 1)):\n",
    "    \"\"\"\n",
    "    Given a list of (url, date) pairs, cuts off all those before a certain period.\n",
    "    \"\"\"\n",
    "    return [link for link in links if link[1] <= cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating Through Each Page\n",
    "\n",
    "Now, we need to go through and process each page one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_indices = [''] + ['?page={}'.format(i) for i in list(range(1, 25))]\n",
    "urls = [url + page_index for page_index in page_indices]\n",
    "pages = [requests.get(url) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = []\n",
    "addresses = []\n",
    "\n",
    "for page in pages:\n",
    "    links = get_links(page)\n",
    "    links = filter_by_date(links)\n",
    "    for link in links:\n",
    "        # Note that the first 20 characters are used for web.archive.org.\n",
    "        if link[0][20:] not in addresses:\n",
    "            addresses.append(link[0][20:])\n",
    "            link_list.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dill.dump(link, open('graph_checkpoints/link_list.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = dill.load(open('graph_checkpoints/link_list.pkd', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Event\n",
    "\n",
    "So, we have multiple links. Each link contains several photos. We're going to go through each page, and grab the names from the captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing Names from Caption\n",
    "\n",
    "First, we need to look at one is in each caption. Let us start with a simple test page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://web.archive.org/web/20151114014941/http://www.newyorksocialdiary.com/party-pictures/2015/celebrating-the-neighborhood'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the first caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = soup.find_all(attrs={'class': 'photocaption'})[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glenn Adamson, Simon Doonan, Victoire de Castellane, Craig Leavitt, Jerome Chazen, Andi Potamkin, Ralph Pucci, Kirsten Bailey, Edwin Hathaway, and Dennis Freedman at the Museum of Art and Design's annual MAD BALL. \n"
     ]
    }
   ],
   "source": [
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest solution to grab all the names is to use a library such as spaCy to parse each caption. Note that we use the large English model for increased accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(text):\n",
    "    \"\"\"\n",
    "    Grabs the names located inside some caption text.\n",
    "    \n",
    "    Keyword Argument:\n",
    "    text - Text we wish to analyze.\n",
    "    \n",
    "    Returns:\n",
    "    - List of names inside text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    names = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            names.append(ent.text.strip())\n",
    "            \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Glenn Adamson', 'Simon Doonan', 'Victoire de Castellane', 'Craig Leavitt', 'Jerome Chazen', 'Andi Potamkin', 'Ralph Pucci', 'Kirsten Bailey', 'Edwin Hathaway', 'Dennis Freedman']\n"
     ]
    }
   ],
   "source": [
    "print(get_names(caption))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to go through the link, listing out all the names associated with each photo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating Through our Link List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_captions(path):\n",
    "    page = requests.get(path)\n",
    "    soup = BeautifulSoup(page.text, \"lxml\")\n",
    "\n",
    "    captions = []\n",
    "    for caption in soup.find_all(attrs={'class': 'photocaption'}):\n",
    "        caption = caption.text\n",
    "        captions.append(caption)\n",
    "                \n",
    "    return captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this may take a considerable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = []\n",
    "\n",
    "for link in link_list:\n",
    "    captions.extend(get_captions('https://web.archive.org' + link[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the time this took, we will cache our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(captions, open('graph_checkpoints/captions.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load our results, simply run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = dill.load(open('graph_checkpoints/captions.pkd', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we only need names out of this text. Note that we could save time by combining this with the previous loop. I nonetheless wished to do them separately to by able to save the results, in case changes needed to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [get_names(caption) for caption in captions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this list contains multiple instances of empty lists, which contain no valuable information. Let us remove these to speed things up a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(captions, open('graph_checkpoints/caption_names.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up\n",
    "\n",
    "Now, there are a couple things that need to be done to clean up. First of all, some captions contain no names. Instead of just ignoring them, Python designates that to be the empty list. We should remove these to speed things up as well as save on RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        captions.remove([])\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have cases where the caption reads 'John and Bonnie Williamson.' These are translated as 'John' and 'Bonnie Williamson', where 'John' implicitly has the last name 'Williamson.' Let's fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for caption in captions:\n",
    "    for i, name in enumerate(caption):\n",
    "        if len(name.split(' ')) == 1:\n",
    "            try:\n",
    "                caption[i] = name + ' ' + caption[i + 1].split(' ')[-1]\n",
    "            except:\n",
    "                caption.remove(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, note one of the most popular names, 'Patrick McMullan,' is actually the photographer. We need to remove his name from all the captions, as they influence results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, caption in enumerate(captions):\n",
    "    try:\n",
    "        caption.remove('Patrick McMullan')\n",
    "        captions[i] = caption\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Construction\n",
    "\n",
    "Now, we wish to construct a graph which we could analyze our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "for caption in captions:\n",
    "    for n, m in combinations(caption, 2):\n",
    "        if G.has_edge(n, m):\n",
    "            G[n][m]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(n, m, weight=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "### Who is the most popular?\n",
    "\n",
    "First, let us define what I mean in this case by popular. I am simply referring to who has the largest number of edges, or who is found next to the largest number of people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = dict()\n",
    "\n",
    "for name in G.nodes():\n",
    "    people[name] = G.degree(name)\n",
    "    \n",
    "people = [\n",
    "    (person, people[person]) for person in people.keys()\n",
    "]\n",
    "\n",
    "people = sorted(people, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see who some of the most popular people are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jean Shafiroff', 453),\n",
       " ('Mark Gilbertson', 352),\n",
       " ('Alexandra Lebenthal', 245),\n",
       " ('Gillian Miniter', 239),\n",
       " ('Geoffrey Bradfield', 228),\n",
       " ('Mario Buatta', 224),\n",
       " ('Michael Bloomberg', 205),\n",
       " ('Eleanora Kennedy', 203),\n",
       " ('Kamie Lightburn', 201),\n",
       " ('Yaz Hernandez', 200),\n",
       " ('Alina Cho', 185),\n",
       " ('Somers Farkas', 184),\n",
       " ('Sharon Bush', 181),\n",
       " ('Lucia Hwong Gordon', 181),\n",
       " ('Andrew Saffir', 181),\n",
       " ('Debbie Bancroft', 167),\n",
       " ('Jamee Gregory', 161),\n",
       " ('Liliana Cavendish', 161),\n",
       " ('Barbara Tober', 160),\n",
       " ('Allison Aston', 159),\n",
       " ('Bettina Zilkha', 148),\n",
       " ('Amy Fine Collins', 145),\n",
       " ('Donna Karan', 145),\n",
       " ('Karen Klopp', 142),\n",
       " ('Leonard Lauder', 142),\n",
       " ('Karen LeFrak', 141),\n",
       " ('Martha Stewart', 139),\n",
       " ('Christopher Hyland', 139),\n",
       " ('Deborah Norville', 137),\n",
       " ('Diana Taylor', 137),\n",
       " ('Ellen V. Futter', 132),\n",
       " ('Jennifer Creel', 130),\n",
       " ('Grace Meigher', 129),\n",
       " ('Margo Langenberg', 129),\n",
       " ('Paula Zahn', 129),\n",
       " ('Lydia Fenet', 129),\n",
       " ('Alec Baldwin', 128),\n",
       " ('Nicole Miller', 126),\n",
       " ('Elizabeth Stribling', 125),\n",
       " ('Liz Peek', 125),\n",
       " ('Bonnie Comley', 125),\n",
       " ('Michele Herbert', 120),\n",
       " ('Audrey Gruss', 118),\n",
       " ('Diana DiMenna', 117),\n",
       " ('Kipton Cronkite', 116),\n",
       " ('Dennis Basso', 113),\n",
       " ('Felicia Taylor', 113),\n",
       " ('Amy Hoadley', 113),\n",
       " ('Annette Rickel', 113),\n",
       " ('Evelyn Lauder', 112)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note a couple of names here are unimportant. For instance, having only first names is an issue. Also, some are specifically titles or credidentials, which while they refer to specific people, are not those individuals in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who the the most connected?\n",
    "\n",
    "This is definitely one of the most ambiguous statements. So, to rank connectedness, I'm going to using PageRank, which originally brought Google on the map.\n",
    "\n",
    "While the technical details can be found elsewhere (for instance, Kevin Murphy's text), the basic idea is as follows. At each node, we look at who is connected to that individual. But each connection is weighed based off how connected those individuals are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "most_connected = [(key, pr[key]) for key in pr.keys()]\n",
    "most_connected = sorted(most_connected, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jean Shafiroff', 0.0007590385181084754),\n",
       " ('Mark Gilbertson', 0.0005258709418796361),\n",
       " ('Gillian Miniter', 0.0004441257514677958),\n",
       " ('Geoffrey Bradfield', 0.0004065273986735837),\n",
       " ('Alexandra Lebenthal', 0.0003970781346492939),\n",
       " ('Mario Buatta', 0.0003436314606364027),\n",
       " ('Yaz Hernandez', 0.0003358716181693452),\n",
       " ('Andrew Saffir', 0.00033150251980629025),\n",
       " ('Kamie Lightburn', 0.00032632104929191123),\n",
       " ('Eleanora Kennedy', 0.00031725877514452144),\n",
       " ('Michael Bloomberg', 0.00030075976754423634),\n",
       " ('Sharon Bush', 0.0002991035986510989),\n",
       " ('Alina Cho', 0.0002934927135324691),\n",
       " ('Barbara Tober', 0.0002884279337231575),\n",
       " ('Somers Farkas', 0.0002861021252845084),\n",
       " ('Debbie Bancroft', 0.0002748729742278528),\n",
       " ('Lucia Hwong Gordon', 0.0002654207603430986),\n",
       " ('Bonnie Comley', 0.00025970619702183007),\n",
       " ('Jamee Gregory', 0.00025473987502650766),\n",
       " ('Liliana Cavendish', 0.00024853807256546066),\n",
       " ('Christopher Hyland', 0.0002363157830562482),\n",
       " ('Karen LeFrak', 0.0002257863228488658),\n",
       " ('Donna Karan', 0.00022433067674585224),\n",
       " ('Elizabeth Stribling', 0.00021893427894561072),\n",
       " ('Amy Fine Collins', 0.00021838688794178376),\n",
       " ('Bettina Zilkha', 0.00021522062780975014),\n",
       " ('Diana Taylor', 0.00021475229304826818),\n",
       " ('Lydia Fenet', 0.00021360021790857122),\n",
       " ('Allison Aston', 0.00021347652558524992),\n",
       " ('Martha Stewart', 0.00020888530501492281),\n",
       " ('Rob Rich', 0.0002054435641074094),\n",
       " ('Deborah Norville', 0.00020427071155117702),\n",
       " ('Alec Baldwin', 0.00020179068592532105),\n",
       " ('Michele Herbert', 0.00020052770831113982),\n",
       " ('Karen Klopp', 0.00019882233577871047),\n",
       " ('Daniel Benedict', 0.0001976152575620636),\n",
       " ('Margo Langenberg', 0.00019467166671789757),\n",
       " ('Dawne Marie Grannum', 0.00019408044264272714),\n",
       " ('Stewart Lane', 0.00019312521919848176),\n",
       " ('Barbara Regna', 0.00019306476844201952),\n",
       " ('Liz Peek', 0.00019140126128114028),\n",
       " ('Russell Simmons', 0.0001898741245516367),\n",
       " ('Nicole Miller', 0.0001887381268258714),\n",
       " ('Leonard Lauder', 0.00018449034092922698),\n",
       " ('Grace Meigher', 0.00018411596388389186),\n",
       " ('Paula Zahn', 0.00017978861762303315),\n",
       " ('Kipton Cronkite', 0.00017960124291344876),\n",
       " ('Fernanda Kellogg', 0.00017771618952516294),\n",
       " ('Dennis Basso', 0.00017752996573308535),\n",
       " ('Amy Hoadley', 0.00017702601597351268)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_connected[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note there is some similarity in the order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who are best friends?\n",
    "\n",
    "Now let's look at who has the strongest edge weights, which indicates that they commonly interact together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_connections_between = []\n",
    "\n",
    "for i, j in G.edges():\n",
    "    num_connections = G[i][j]['weight']\n",
    "    if num_connections > 10:\n",
    "        number_of_connections_between.append(((i, j), num_connections))\n",
    "        \n",
    "number_of_connections_between = sorted(number_of_connections_between, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Gillian Miniter', 'Sylvester Miniter'), 117),\n",
       " (('Bonnie Comley', 'Stewart Lane'), 83),\n",
       " (('Jamee Gregory', 'Peter Gregory'), 77),\n",
       " (('Geoffrey Bradfield', 'Roric Tobin'), 69),\n",
       " (('Daniel Benedict', 'Andrew Saffir'), 66),\n",
       " (('Barbara Tober', 'Donald Tober'), 57),\n",
       " (('Jean Shafiroff', 'Martin Shafiroff'), 56),\n",
       " (('Eleanora Kennedy', 'Michael Kennedy'), 50),\n",
       " (('Alexandra Lebenthal', 'Jay Diamond'), 47),\n",
       " (('Peter Regna', 'Barbara Regna'), 46)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_connections_between[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a side note, most of the highest entries here are referencing a lot of celebraty couples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
